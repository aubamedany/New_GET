{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, '../GET')\n",
    "\n",
    "\n",
    "from Models.FCWithEvidences import graph_based_semantic_structure\n",
    "from Fitting.FittingFC import char_man_fitter_query_repr1\n",
    "import time\n",
    "import json\n",
    "from interactions import ClassificationInteractions\n",
    "import matchzoo as mz\n",
    "from handlers import cls_load_data\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_utils\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from handlers.output_handler_FC import FileHandlerFC\n",
    "from Evaluation import mzEvaluator as evaluator\n",
    "from setting_keywords import KeyWordSettings\n",
    "from matchzoo.embedding import entity_embedding\n",
    "from Models.BiDAF.wrapper import GGNN, GGNN_with_GSL, Linear\n",
    "from thirdparty.two_branches_attention import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fccd0991570>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=\"PolitiFact\"\n",
    "fixed_length_left=30\n",
    "fixed_length_right=100\n",
    "log=\"logs/get\"\n",
    "loss_type=\"cross_entropy\"\n",
    "batch_size=32\n",
    "num_folds=5\n",
    "use_claim_source=1\n",
    "use_article_source=1\n",
    "path=\"formatted_data/declare/\"\n",
    "hidden_size=300\n",
    "epochs=100\n",
    "num_att_heads_for_words=3\n",
    "num_att_heads_for_evds=1\n",
    "gnn_window_size=3\n",
    "lr=0.0001\n",
    "gnn_dropout=0.2\n",
    "seed=123456\n",
    "gsl_rate=0.6\n",
    "fixed_length_left_src_chars = 20\n",
    "fixed_length_right_src_chars = 20\n",
    "seed = 123456\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = mz.preprocessors.CharManPreprocessor(fixed_length_left = fixed_length_left,\n",
    "                                                    fixed_length_right = fixed_length_right,\n",
    "                                                    fixed_length_left_src = fixed_length_left_src_chars,\n",
    "                                                    fixed_length_right_src = fixed_length_right_src_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_data ={}\n",
    "def get_graph(dataset=\"PolitiFact\",fixed_length_left=30,fixed_length_right=100,log=\"logs/get\",batch_size=32,gnn_window_size=3,fold_idx=4):\n",
    "  if not os.path.exists(log):\n",
    "        os.mkdir(log)\n",
    "  secondary_log_folder = os.path.join(log, \"log_results_%s\" % (dataset))\n",
    "  if not os.path.exists(secondary_log_folder):\n",
    "    os.mkdir(secondary_log_folder)\n",
    "  secondary_log_folder = secondary_log_folder\n",
    "  root = os.path.join(os.path.join(path,dataset), \"mapped_data\")\n",
    "  tx = time.time()\n",
    "  kfold_dev_results, kfold_test_results = [], []\n",
    "  list_metrics = KeyWordSettings.CLS_METRICS\n",
    "  outfolder_per_fold = os.path.join(secondary_log_folder, \"Fold_%s\" % fold_idx)\n",
    "  if not os.path.exists(outfolder_per_fold):\n",
    "      os.mkdir(outfolder_per_fold)\n",
    "  logfolder_result_per_fold = os.path.join(outfolder_per_fold, \"result_%s.txt\" % int(seed))\n",
    "  file_handler = FileHandlerFC()\n",
    "  file_handler.init_log_files(logfolder_result_per_fold)\n",
    "  # root =\"/content/drive/MyDrive/GET/formatted_data/declare/Snopes/mapped_data\"\n",
    "  # ------------------------------------------------------------------------------------\n",
    "  predict_pack = cls_load_data.load_data(root + \"/%sfold\" % num_folds, 'test_%s' % '4', kfolds = num_folds)\n",
    "  train_pack = cls_load_data.load_data(root + \"/%sfold\" % num_folds, 'train_%sres' % '4', kfolds = num_folds)\n",
    "  valid_pack = cls_load_data.load_data(root, 'dev', kfolds = num_folds)\n",
    "\n",
    "  a = train_pack.left[\"text_left\"].str.lower().str.split().apply(len).max()\n",
    "  b = valid_pack.left[\"text_left\"].str.lower().str.split().apply(len).max()\n",
    "  c = predict_pack.left[\"text_left\"].str.lower().str.split().apply(len).max()\n",
    "  max_query_length = max([a, b, c])\n",
    "  min_query_length = min([a, b, c])\n",
    "\n",
    "  a = train_pack.right[\"text_right\"].str.lower().str.split().apply(len).max()\n",
    "  b = valid_pack.right[\"text_right\"].str.lower().str.split().apply(len).max()\n",
    "  c = predict_pack.right[\"text_right\"].str.lower().str.split().apply(len).max()\n",
    "  max_doc_length = max([a, b, c])\n",
    "  min_doc_length = min([a, b, c])\n",
    "\n",
    "  file_handler.myprint(\"Min query length, \" + str(min_query_length) + \" Min doc length \" + str(min_doc_length))\n",
    "  file_handler.myprint(\"Max query length, \" + str(max_query_length) + \" Max doc length \" + str(max_doc_length))\n",
    "  global additional_data\n",
    "  additional_data = {KeyWordSettings.OutputHandlerFactChecking: file_handler,\n",
    "                      KeyWordSettings.GNN_Window: gnn_window_size}\n",
    "\n",
    "  print('parsing data')\n",
    "\n",
    "  train_processed = preprocessor.fit_transform(train_pack)  # This is a DataPack\n",
    "  valid_processed = preprocessor.transform(valid_pack)\n",
    "  predict_processed = preprocessor.transform(predict_pack)\n",
    "\n",
    "\n",
    "  train_interactions = ClassificationInteractions(train_processed, **additional_data)\n",
    "  valid_interactions = ClassificationInteractions(valid_processed, **additional_data)\n",
    "  predict_interactions = ClassificationInteractions(predict_processed, **additional_data)\n",
    "  file_handler.myprint('done extracting')\n",
    "  return train_processed, valid_processed, predict_processed, train_interactions, valid_interactions, predict_interactions\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5624/5624 [00:00<00:00, 72263.97it/s]\n",
      "100%|██████████| 5624/5624 [00:00<00:00, 664397.41it/s]\n",
      "100%|██████████| 20824/20824 [00:00<00:00, 79601.50it/s]\n",
      "100%|██████████| 20824/20824 [00:00<00:00, 792866.62it/s]\n",
      "100%|██████████| 3108/3108 [00:00<00:00, 78303.55it/s]\n",
      "100%|██████████| 3108/3108 [00:00<00:00, 763672.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min query length, 24 Min doc length 100\n",
      "Max query length, 25 Max doc length 100\n",
      "parsing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 2570/2570 [00:00<00:00, 11018.46it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 20824/20824 [00:07<00:00, 2972.62it/s]\n",
      "Processing text_left with extend: 100%|██████████| 2570/2570 [00:00<00:00, 1024264.66it/s]\n",
      "Processing text_right with extend: 100%|██████████| 20824/20824 [00:00<00:00, 810592.82it/s]\n",
      "Building Vocabulary from a datapack.: 100%|██████████| 2093184/2093184 [00:00<00:00, 4343462.79it/s]\n",
      "Processing text_right with extend: 100%|██████████| 2570/2570 [00:00<00:00, 942828.77it/s]\n",
      "Building Entities Vocabulary from a datapack.: 100%|██████████| 2570/2570 [00:00<00:00, 3922620.55it/s]\n",
      "Processing text_right with extend: 100%|██████████| 20824/20824 [00:00<00:00, 1088335.47it/s]\n",
      "Building Entities Vocabulary from a datapack.: 100%|██████████| 20824/20824 [00:00<00:00, 4454415.88it/s]\n",
      "Processing text_right with extend: 100%|██████████| 2570/2570 [00:00<00:00, 562714.62it/s]\n",
      "Processing text_right with extend: 100%|██████████| 20824/20824 [00:00<00:00, 631856.72it/s]\n",
      "Building Characters Vocabulary from a datapack.: 100%|██████████| 313599/313599 [00:00<00:00, 4387781.14it/s]\n",
      "Processing text_right with extend: 100%|██████████| 2570/2570 [00:00<00:00, 665762.54it/s]\n",
      "Processing text_right with extend: 100%|██████████| 2570/2570 [00:00<00:00, 346960.26it/s]\n",
      "Processing text_right with extend: 100%|██████████| 2570/2570 [00:00<00:00, 179506.43it/s]\n",
      "Processing text_right with extend: 100%|██████████| 20824/20824 [00:00<00:00, 755705.60it/s]\n",
      "Processing text_right with extend: 100%|██████████| 20824/20824 [00:00<00:00, 121529.71it/s]\n",
      "Processing text_right with extend: 100%|██████████| 20824/20824 [00:00<00:00, 195391.14it/s]\n",
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 2570/2570 [00:00<00:00, 11338.37it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 20824/20824 [00:07<00:00, 2630.04it/s]\n",
      "Processing text_left with transform: 100%|██████████| 2570/2570 [00:00<00:00, 225339.94it/s]\n",
      "Processing text_right with transform: 100%|██████████| 20824/20824 [00:00<00:00, 43874.94it/s]\n",
      "Processing length_left with len: 100%|██████████| 2570/2570 [00:00<00:00, 1036676.41it/s]\n",
      "Processing length_right with len: 100%|██████████| 20824/20824 [00:00<00:00, 1156559.10it/s]\n",
      "Processing text_left with transform: 100%|██████████| 2570/2570 [00:00<00:00, 134302.18it/s]\n",
      "Processing text_right with transform: 100%|██████████| 20824/20824 [00:00<00:00, 66382.51it/s]\n",
      "Processing text_right with transform: 100%|██████████| 356/356 [00:00<00:00, 421680.94it/s]\n",
      "Processing text_right with transform: 100%|██████████| 356/356 [00:00<00:00, 244662.01it/s]\n",
      "Processing text_right with transform: 100%|██████████| 356/356 [00:00<00:00, 168968.23it/s]\n",
      "Processing text_right with transform: 100%|██████████| 3108/3108 [00:00<00:00, 283555.50it/s]\n",
      "Processing text_right with transform: 100%|██████████| 3108/3108 [00:00<00:00, 26725.39it/s]\n",
      "Processing text_right with transform: 100%|██████████| 3108/3108 [00:00<00:00, 175494.36it/s]\n",
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 356/356 [00:00<00:00, 9965.38it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 3108/3108 [00:01<00:00, 2999.81it/s]\n",
      "Processing text_left with transform: 100%|██████████| 356/356 [00:00<00:00, 174578.77it/s]\n",
      "Processing text_right with transform: 100%|██████████| 3108/3108 [00:00<00:00, 50202.75it/s]\n",
      "Processing length_left with len: 100%|██████████| 356/356 [00:00<00:00, 650903.32it/s]\n",
      "Processing length_right with len: 100%|██████████| 3108/3108 [00:00<00:00, 1081458.17it/s]\n",
      "Processing text_left with transform: 100%|██████████| 356/356 [00:00<00:00, 174680.89it/s]\n",
      "Processing text_right with transform: 100%|██████████| 3108/3108 [00:00<00:00, 63015.84it/s]\n",
      "Processing text_right with transform: 100%|██████████| 642/642 [00:00<00:00, 543220.33it/s]\n",
      "Processing text_right with transform: 100%|██████████| 642/642 [00:00<00:00, 274154.26it/s]\n",
      "Processing text_right with transform: 100%|██████████| 642/642 [00:00<00:00, 187673.76it/s]\n",
      "Processing text_right with transform: 100%|██████████| 5624/5624 [00:00<00:00, 498231.40it/s]\n",
      "Processing text_right with transform: 100%|██████████| 5624/5624 [00:00<00:00, 335444.12it/s]\n",
      "Processing text_right with transform: 100%|██████████| 5624/5624 [00:00<00:00, 200508.02it/s]\n",
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 642/642 [00:00<00:00, 11558.38it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 5624/5624 [00:01<00:00, 3022.17it/s]\n",
      "Processing text_left with transform: 100%|██████████| 642/642 [00:00<00:00, 225579.56it/s]\n",
      "Processing text_right with transform: 100%|██████████| 5624/5624 [00:00<00:00, 41230.38it/s]\n",
      "Processing length_left with len: 100%|██████████| 642/642 [00:00<00:00, 737940.03it/s]\n",
      "Processing length_right with len: 100%|██████████| 5624/5624 [00:00<00:00, 1086989.80it/s]\n",
      "Processing text_left with transform: 100%|██████████| 642/642 [00:00<00:00, 152485.60it/s]\n",
      "Processing text_right with transform: 100%|██████████| 5624/5624 [00:00<00:00, 57624.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting DataFrame to Normal Dictionary of Data\n",
      "[NOTICE] MatchZoo use queryID and docID as index in dataframe left and right, therefore, iterrows will return index which is left_id or right_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/namle/Library/CloudStorage/GoogleDrive-hcmutepfl@gmail.com/My Drive/GET/interactions.py:15: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] MatchZoo use queryID and docID as index in dataframe left and right, therefore, iterrows will return index which is left_id or right_id\n",
      "Converting DataFrame to Normal Dictionary of Data\n",
      "[NOTICE] MatchZoo use queryID and docID as index in dataframe left and right, therefore, iterrows will return index which is left_id or right_id\n",
      "[NOTICE] MatchZoo use queryID and docID as index in dataframe left and right, therefore, iterrows will return index which is left_id or right_id\n",
      "Converting DataFrame to Normal Dictionary of Data\n",
      "[NOTICE] MatchZoo use queryID and docID as index in dataframe left and right, therefore, iterrows will return index which is left_id or right_id\n",
      "[NOTICE] MatchZoo use queryID and docID as index in dataframe left and right, therefore, iterrows will return index which is left_id or right_id\n",
      "done extracting\n"
     ]
    }
   ],
   "source": [
    "train_processed, valid_processed, predict_processed, train_interactions, valid_interactions, predict_interactions = get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embeddings......\n",
      "Word hit: (32051, 32053) 99.99376033444607\n",
      "Time to load word embeddings...... 8.371541976928711\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading word embeddings......\")\n",
    "t1_emb = time.time()\n",
    "term_index = preprocessor.context['vocab_unit'].state['term_index']\n",
    "glove_embedding = mz.datasets.embeddings.load_glove_embedding_FC(dimension = 300,\n",
    "                                                                  term_index = term_index, **additional_data)\n",
    "\n",
    "embedding_matrix = glove_embedding.build_matrix(term_index)\n",
    "entity_embs1 = entity_embedding.EntityEmbedding(128)\n",
    "claim_src_embs_matrix = entity_embs1.build_matrix(preprocessor.context['claim_source_unit'].state['term_index'])\n",
    "\n",
    "entity_embs2 = entity_embedding.EntityEmbedding(128)\n",
    "article_src_embs_matrix = entity_embs2.build_matrix(preprocessor.context['article_source_unit'].state['term_index'])\n",
    "\n",
    "t2_emb = time.time()\n",
    "print(\"Time to load word embeddings......\", (t2_emb - t1_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32053, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_params = {}\n",
    "match_params['embedding'] = embedding_matrix\n",
    "match_params[\"num_classes\"] = 2\n",
    "match_params[\"fixed_length_right\"] = 100\n",
    "match_params[\"fixed_length_left\"] = 30\n",
    "\n",
    "# for claim source\n",
    "match_params[\"use_claim_source\"] = use_claim_source\n",
    "match_params[\"claim_source_embeddings\"] = claim_src_embs_matrix\n",
    "# for article source\n",
    "match_params[\"use_article_source\"] = use_article_source\n",
    "match_params[\"article_source_embeddings\"] = article_src_embs_matrix\n",
    "# multi-head attention\n",
    "match_params[\"cuda\"] = 0\n",
    "match_params[\"num_att_heads_for_words\"] = num_att_heads_for_words  # first level\n",
    "match_params[\"num_att_heads_for_evds\"] = num_att_heads_for_evds  # second level\n",
    "\n",
    "\n",
    "match_params['dropout_gnn'] = 0.2\n",
    "match_params[\"dropout_left\"] = 0.2\n",
    "match_params[\"dropout_right\"] = 0.2\n",
    "match_params[\"hidden_size\"] = hidden_size\n",
    "\n",
    "match_params[\"gsl_rate\"] = 0.6\n",
    "\n",
    "match_params[\"embedding_freeze\"] = True\n",
    "match_params[\"output_size\"] = 2 # if args.dataset == \"Snopes\" else 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_model = graph_based_semantic_structure.Graph_basedSemantiStructure(match_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_model.load_state_dict(torch.load(\"/Users/namle/Library/CloudStorage/GoogleDrive-hcmutepfl@gmail.com/My Drive/GET/logs/get/log_results_PolitiFact_0.05/Fold_4/saved_model_123456\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from handlers.output_handler_FC import FileHandlerFC\n",
    "loss_type = 'cross_entropy'\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "early_stopping = 10\n",
    "outfolder_per_fold = \"/Users/namle/Library/CloudStorage/GoogleDrive-hcmutepfl@gmail.com/My Drive/GET/logs/get/log_results_Snopes/Fold_10\"\n",
    "curr_date = datetime.datetime.now().timestamp()\n",
    "fixed_num_evidences = 30\n",
    "file_handler = FileHandlerFC()\n",
    "seed = 123456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = char_man_fitter_query_repr1.CharManFitterQueryRepr1(net = match_model, loss = loss_type, n_iter = epochs,\n",
    "                                                  batch_size = batch_size, learning_rate = lr,\n",
    "                                                  early_stopping = early_stopping, use_cuda = 0,\n",
    "                                                  logfolder = outfolder_per_fold, curr_date = curr_date,\n",
    "                                                  fixed_num_evidences = fixed_num_evidences,\n",
    "                                                  output_handler_fact_checking = file_handler, seed=seed,\n",
    "                                                  output_size=match_params[\"output_size\"],args=\"args\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ids, left_contents, left_lengths, query_sources, query_char_sources, query_adj, \\\n",
    "            evd_docs_ids, evd_docs_contents, evd_docs_lens, evd_sources, evd_cnt_each_query, evd_char_sources, \\\n",
    "            pair_labels, evd_docs_adj = fit_model._sampler.get_train_instances_char_man(valid_interactions,\n",
    "                                                                                   fit_model.fixed_num_evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 4\n",
    "query_ids = query_ids[:size]\n",
    "left_contents = left_contents[:size]\n",
    "left_lengths = left_lengths[:size]\n",
    "query_sources = query_sources[:size]\n",
    "query_char_sources = query_char_sources[:size]\n",
    "query_adj = query_adj[:size]\n",
    "evd_docs_ids = evd_docs_ids[:size]\n",
    "evd_docs_contents = evd_docs_contents[:size]\n",
    "evd_docs_lens = evd_docs_lens[:size]\n",
    "evd_sources = evd_sources[:size]\n",
    "evd_cnt_each_query = evd_cnt_each_query[:size]\n",
    "evd_char_sources = evd_char_sources[:size]\n",
    "pair_labels = pair_labels[:size]\n",
    "evd_docs_adj = evd_docs_adj[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "import torch_utils as my_utils\n",
    "query_ids = my_utils.gpu(torch.from_numpy(query_ids), cuda)\n",
    "left_contents = my_utils.gpu(torch.from_numpy(left_contents), cuda)\n",
    "# left_lengths = my_utils.gpu(torch.from_numpy(left_lengths), cuda)\n",
    "query_sources = my_utils.gpu(torch.from_numpy(query_sources), cuda)\n",
    "query_char_sources = my_utils.gpu(torch.from_numpy(query_char_sources), cuda)\n",
    "query_adj = my_utils.gpu(torch.from_numpy(query_adj), cuda)\n",
    "\n",
    "evd_docs_ids = my_utils.gpu(torch.from_numpy(evd_docs_ids), cuda)\n",
    "evd_docs_contents = my_utils.gpu(torch.from_numpy(evd_docs_contents), cuda)\n",
    "# evd_docs_lens = my_utils.gpu(torch.from_numpy(evd_docs_lens), cuda)\n",
    "evd_sources = my_utils.gpu(torch.from_numpy(evd_sources), cuda)\n",
    "evd_cnt_each_query = my_utils.gpu(torch.from_numpy(evd_cnt_each_query), cuda)\n",
    "evd_char_sources = my_utils.gpu(torch.from_numpy(evd_char_sources), cuda)\n",
    "\n",
    "pair_labels = my_utils.gpu(torch.from_numpy(pair_labels), cuda)\n",
    "evd_docs_adj = my_utils.gpu(torch.from_numpy(evd_docs_adj), cuda)\n",
    "# total_pairs += self._batch_size * self.\n",
    "additional_data = {KeyWordSettings.EvidenceCountPerQuery: evd_cnt_each_query,\n",
    "                    KeyWordSettings.FCClass.QueryCharSource: query_char_sources,\n",
    "                    KeyWordSettings.FCClass.DocCharSource: evd_char_sources,\n",
    "                    KeyWordSettings.Query_Adj: query_adj,\n",
    "                    KeyWordSettings.Evd_Docs_Adj: evd_docs_adj}\n",
    "n=30\n",
    "evd_count_per_query = evd_cnt_each_query  # (B, )\n",
    "query_char_source = query_char_sources\n",
    "doc_char_source = evd_char_sources\n",
    "query_adj = query_adj\n",
    "evd_docs_adj = evd_docs_adj\n",
    "# assert evd_doc_ids.size() == evd_docs_lens.shape\n",
    "# assert query_ids.size(0) == evd_doc_ids.size(0)\n",
    "# assert query_lens.shape == labels.size()\n",
    "# assert query_contents.size(0) == evd_doc_contents.size(0)  # = batch_size\n",
    "_, L = left_contents.size()\n",
    "batch_size = query_ids.size(0)\n",
    "# prunning at this step to remove padding\\\n",
    "e_lens, e_conts, q_conts, q_lens, e_adj = [], [], [], [], []\n",
    "e_chr_src_conts = []\n",
    "expaned_labels = []\n",
    "for evd_cnt, q_cont, q_len, evd_lens, evd_doc_cont, evd_chr_src, label, evd_adj in \\\n",
    "        zip(evd_count_per_query, left_contents, left_lengths,\n",
    "            evd_docs_lens, evd_docs_contents, doc_char_source, pair_labels, evd_docs_adj):\n",
    "    evd_cnt = int(torch_utils.cpu(evd_cnt).detach().numpy())\n",
    "    e_lens.extend(list(evd_lens[:evd_cnt]))\n",
    "    e_conts.append(evd_doc_cont[:evd_cnt, :])  # stacking later\n",
    "    e_adj.append(evd_adj[:evd_cnt])\n",
    "    e_chr_src_conts.append(evd_chr_src[:evd_cnt, :])\n",
    "    q_lens.extend([q_len] * evd_cnt)\n",
    "    q_conts.append(q_cont.unsqueeze(0).expand(evd_cnt, L))\n",
    "    expaned_labels.extend([int(torch_utils.cpu(label).detach().numpy())] * evd_cnt)\n",
    "# concat\n",
    "e_conts = torch.cat(e_conts, dim=0)  # (n1 + n2 + ..., R)\n",
    "e_chr_src_conts = torch.cat(e_chr_src_conts, dim=0)  # (n1 + n2 + ... , R)\n",
    "e_adj = torch.cat(e_adj, dim=0)     # (n1 + n2 + ..., R, R)\n",
    "e_lens = np.array(e_lens)  # (n1 + n2 + ..., )\n",
    "q_conts = torch.cat(q_conts, dim=0)  # (n1 + n2 + ..., R)\n",
    "q_lens = np.array(q_lens)\n",
    "assert q_conts.size(0) == q_lens.shape[0] == e_conts.size(0) == e_lens.shape[0]\n",
    "\n",
    "d_new_indices, d_old_indices = torch_utils.get_sorted_index_and_reverse_index(e_lens)\n",
    "e_lens = my_utils.gpu(torch.from_numpy(e_lens), cuda)\n",
    "x = left_lengths\n",
    "q_new_indices, q_restoring_indices = torch_utils.get_sorted_index_and_reverse_index(x)\n",
    "x = my_utils.gpu(torch.from_numpy(x), cuda)\n",
    "# query_lens = my_utils.gpu(torch.from_numpy(query_lens), self._use_cuda)\n",
    "\n",
    "additional_paramters = {\n",
    "    KeyWordSettings.Query_lens: x,  # 每一个query长度\n",
    "    KeyWordSettings.Doc_lens: evd_docs_lens,\n",
    "    KeyWordSettings.DocLensIndices: (d_new_indices, d_old_indices, e_lens),\n",
    "    KeyWordSettings.QueryLensIndices: (q_new_indices, q_restoring_indices, x),\n",
    "    KeyWordSettings.QuerySources: query_sources,\n",
    "    KeyWordSettings.DocSources: evd_sources,\n",
    "    KeyWordSettings.TempLabel: pair_labels,\n",
    "    KeyWordSettings.DocContentNoPaddingEvidence: e_conts,\n",
    "    KeyWordSettings.QueryContentNoPaddingEvidence: q_conts,\n",
    "    KeyWordSettings.EvidenceCountPerQuery: evd_count_per_query,\n",
    "    KeyWordSettings.FCClass.QueryCharSource: query_char_source,  # (B, 1, L)\n",
    "    KeyWordSettings.FCClass.DocCharSource: e_chr_src_conts,\n",
    "    KeyWordSettings.FIXED_NUM_EVIDENCES: n,\n",
    "    KeyWordSettings.Query_Adj: query_adj,\n",
    "    KeyWordSettings.Evd_Docs_Adj: e_adj                       # flatten->(n1 + n2 ..., R, R)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = match_model.predict(left_contents[:1], evd_docs_contents[:1], **additional_paramters)\n",
    "phi,output,avg,query_repr,doc_out_ggnn,query_repr1,avg1 = match_model.predict(left_contents, evd_docs_contents, **additional_paramters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Dot product\n",
    "result = torch.dot(tensor1, tensor2)\n",
    "print(result)  # Output: tensor(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
